{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "우리가 알거니와 하나님을 사랑하는 자 곧 그의 뜻대로 부르심을 입은 자들에게는 모든 것이 합력하여 선을 이루느니라. 롬8:28  \n",
    "\n",
    "And we know that in all things God works for the good of those who love him, who have been called according to his purpose. (Rom 8:28)\n",
    "\n",
    "-------\n",
    "\n",
    "# Welcome to \"AI for All\"\n",
    "\n",
    "Lecture Notes by idebtor@gmail.com, Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 1 강 신경망을 내 손으로 만져보기(Gate)\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. AND 연산 \n",
    "\n",
    "AND 연산은 모두 입력이 참일 때, 참을 결과로 출력하는 연산입니다.  다음의 진리표에 표시된 바와 같습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/truthtable.png?raw=true\" width=\"400\">\n",
    "<center>그림 1:  OR, AND, NAND, XOR 진리표 </center>\n",
    "\n",
    "\n",
    "이를 넘파이 배열로 나타내면 다음과 같습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계 학습의 관점으로 AND 로직 연산을 바라보면, 입력은 두 개 특성($x_1, x_2$)을 가지고 있으며, 네 개의 샘플이 있으며, 따라서 네 개의 각 샘플에 대한 레이블(혹은 타깃)이 있습니다. 이러한 샘플들과 레이블이 곧 퍼셉트론의 훈련 데이터셋이 됩니다. \n",
    "\n",
    "우리는 사실상 위에서 훈련 데이터셋을 준비한 것입니다. 테스트 혹은 예측을 위한 데이터셋은 존재하지 않으므로, 입력 데이터셋을 그대로 테스트를 위해서도 사용할 것입니다. \n",
    "\n",
    "다음은 데이터셋의 샘플 갯수와 레이블 갯수가 같은지 점검하고, 샘플 갯수와 에폭을 설정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x) == len(y)\n",
    "samples = len(x)\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키기 전에, 가중치와 편향도 정규 분포를 가지는 난수로 초기화 해줍니다. 또한, 학습률(eta)은 0.1로 설정하였습니다.  입력값(특성)이 두개이므로, 가중치도 각각에 맞춰서 두개를 선언해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                 # import tensorflow as tf \n",
    "w = np.array([1.0, 1.0])            # w = tf.random.normal([2], 0, 1)         # mean = 0, std dev = 1\n",
    "b = np.array([1.0])                 # b = tf.random.normal([1], 0, 1) \n",
    "eta = 0.1                           # 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습을 시켜보겠습니다. 앞에서 만든 편향을 가진 뉴런처럼 코드를 구성하면 됩니다. 각각 네가지 경우를 한번씩 학습할 때마다, 네가지 를 예측한 값과 실제 값의 차이로 계산할 수 있는 SSE(sum of squared error)를 구해서 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):  \n",
    "    return 1/(1 + np.exp(-x))         # sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [0.0023528]\n",
      "200 [0.00116179]\n",
      "300 [0.00068577]\n",
      "400 [0.00044865]\n",
      "500 [0.00031457]\n",
      "600 [0.0002319]\n",
      "700 [0.00017757]\n",
      "800 [0.00014008]\n",
      "900 [0.00011317]\n",
      "1000 [9.32504265e-05]\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs): \n",
    "    sse_i = 0 \n",
    "    for j in range(samples): \n",
    "        z = np.sum(x[j] * w) + b        # net input\n",
    "        a = activation(z)               # apply activation function\n",
    "        error = y[j][0] - a             # error\n",
    "        w = w + eta * error * x[j]      # new weight\n",
    "        b = b + eta * error             # new bias\n",
    "        sse_i += error * error\n",
    "    if (i + 1) % 100 == 0: \n",
    "        print(i + 1, sse_i / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 학습을 통해서 결정된 것은 무엇입니까? \n",
    "\n",
    "모델의 가중치와 편향입니다.  \n",
    "\n",
    "이제 학습을 통해서 얻은 기계 학습 모델에 새로운 데이터를 제공하여 출력을 예측하고자 합니다. 다만, 여기서는 예측에 사용할 데이터가 없으므로, 학습에 이미 사용한 데이터를 사용하여 각각의 케이스를 예측하는 것이 합당합니다. 이를 계산하는 방법은 다음과 같이 퍼셉트론의 __순입력(z)__를 구하고 이를 __활성화 함수에 적용하면__ 예측값을 계산할 수 있습니다. __즉 모델의 순뱡향(forpass)만 계산하면 예측값을 산출할 수 있습니다.__ 이러한 두 연산을 합하여 forpass() 메소드로 구현합니다. \n",
    "```\n",
    "z = np.sum(x[i] * w) + b \n",
    "a = activation(z)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [0] y_hat:[0.00018617]\n",
      "[0 1] [0] y_hat:[0.04842622]\n",
      "[1 0] [0] y_hat:[0.04870589]\n",
      "[1 1] [1] y_hat:[0.93330356]\n"
     ]
    }
   ],
   "source": [
    "for i in range(samples): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y[i], activation(np.sum(x[i] * w) + b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이, 실제로 1이 나와야하는 값은 1에 가깝게, 0에 가까워야하는 값은 0에 가깝게 예측이 되었습니다. `[0 0]` 케이스의 경우에는 다른 케이스들 보다 더 0에 가까운걸로 보아 더 확실한 케이스라는 점을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OR 연산\n",
    "\n",
    "OR 연산은 입력들 중에 하나라도 참이면, 참을 결과로 출력하는 연산입니다.  이를 넘파이 배열로 나타내면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 1], [1, 0], [0, 1], [0, 0]], \"float32\") \n",
    "y = np.array([[1], [1], [1], [0]],  \"float32\")\n",
    "\n",
    "w = np.array([1.0, 1.0])            # w = tf.random.normal([2], 0, 1)         # mean = 0, std dev = 1\n",
    "b = np.array([1.0])                 # b = tf.random.normal([1], 0, 1) \n",
    "eta = 0.1                           # 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 AND와 같은 가중치, 편향, 학습률 및 학습 모델로 학습을 시키면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [0.00141742]\n",
      "200 [0.00059027]\n",
      "300 [0.0003127]\n",
      "400 [0.00019075]\n",
      "500 [0.00012751]\n",
      "600 [9.08519009e-05]\n",
      "700 [6.78295892e-05]\n",
      "800 [5.24802686e-05]\n",
      "900 [4.1759897e-05]\n",
      "1000 [3.39897801e-05]\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs): \n",
    "    sse_i = 0 \n",
    "    for j in range(samples): \n",
    "        z = np.sum(x[j] * w) + b        # net input\n",
    "        a = activation(z)               # apply activation function\n",
    "        error = y[j][0] - a             # error\n",
    "        w = w + eta * error * x[j]      # new weight\n",
    "        b = b + eta * error             # new bias\n",
    "        sse_i += error * error\n",
    "    if (i + 1) % 100 == 0: \n",
    "        print(i + 1, sse_i / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행할 수록 error값의 합이 0에 가까워지는것을 확인할 수 있습니다. 각각의 케이스들을 해당 가중치와 편향으로 예측해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.] [1.] y_hat:[0.99997728]\n",
      "[1. 0.] [1.] y_hat:[0.97979667]\n",
      "[0. 1.] [1.] y_hat:[0.97972554]\n",
      "[0. 0.] [0.] y_hat:[0.05054725]\n"
     ]
    }
   ],
   "source": [
    "for i in range(samples): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y[i], activation(np.sum(x[i] * w) + b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XOR 연산\n",
    "XOR은 AND나 OR연산과는 다르게 홀수 개의 입력이 참일 때, 결과가 참이 됩니다. 입력을 두개라고 한다면, 위의 진리표에 나타난 바와 같습니다.\n",
    "\n",
    "이를 넘파이 배열로 나타내면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 1], [1, 0], [0, 1], [0, 0]],  \"float32\") \n",
    "y = np.array([[0], [1], [1], [0]],  \"float32\")\n",
    "\n",
    "w = np.array([1.0, 1.0])            # w = tf.random.normal([2], 0, 1)         # mean = 0, std dev = 1\n",
    "b = np.array([1.0])                 # b = tf.random.normal([1], 0, 1) \n",
    "eta = 0.1                           # 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 AND와 같은 가중치, 편향, 학습률 및 학습 모델로 학습을 시키면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [0.01053073]\n",
      "200 [0.01052111]\n",
      "300 [0.01051963]\n",
      "400 [0.01051935]\n",
      "500 [0.01051929]\n",
      "600 [0.01051928]\n",
      "700 [0.01051928]\n",
      "800 [0.01051928]\n",
      "900 [0.01051928]\n",
      "1000 [0.01051928]\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs): \n",
    "    sse_i = 0 \n",
    "    for j in range(samples): \n",
    "        z = np.sum(x[j] * w) + b        # net input\n",
    "        a = activation(z)               # apply activation function\n",
    "        error = y[j][0] - a             # error\n",
    "        w = w + eta * error * x[j]      # new weight\n",
    "        b = b + eta * error             # new bias\n",
    "        sse_i += error * error\n",
    "    if (i + 1) % 100 == 0: \n",
    "        print(i + 1, sse_i / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이, 특정 학습 지점부터 에러 값에 변화가 거의 없습니다. 이것이 과연 학습이 잘된걸까요? 계산된 가중치와 편향을 가지고 각 케이스를 계산해보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.] [0.] y_hat:[0.51281764]\n",
      "[1. 0.] [1.] y_hat:[0.51281763]\n",
      "[0. 1.] [1.] y_hat:[0.5]\n",
      "[0. 0.] [0.] y_hat:[0.49999999]\n"
     ]
    }
   ],
   "source": [
    "for i in range(samples): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y[i], activation(np.sum(x[i] * w) + b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이, 결과가 0 혹은 1에 가까운 값이 나와 하지만, 모든 케이스가 0.5에 가깝게 나오고 있습니다. 이것은 원하던 결과가 아닙니다. 결과를 해석해보자면, 가중치와 편향 값은 모두 케이스 순서에 의존적이 된다는 것을 알 수 있습니다. 먼저 들어간 `[1 1]`이라는 케이스가 네번째에 들어가는 `[0 0]`이라는 케이스보다 영향을 준다는 것입니다. `[1 1]`이라는 케이스가 먼저 들어가서 가중치와 편향에 중대한 영향을 미치고 이 값들을 가지고 학습을 진행한다는 것이 문제입니다.\n",
    "\n",
    "그러면 XOR문제는 풀지 못하는 걸까요? 여러 층의 퍼셉트론을 사용하면 해결이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 단층 퍼셉트론에서의 XOR 문제점\n",
    "\n",
    "인공 신경망에서는 단층 퍼셉트론으로 XOR 연산이 불가능하다는 것은 마빈 민스키 등에 의해서 밝혀졌습니다. 이러한 내용이 밝혀지면서 인공지능의 겨울이 찾아왔었습니다. 그야말로 전설같은 이야기로 잘 알려져 있습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-history.jpg?raw=true\" width=\"900\">\n",
    "<center>그림 2: XOR 문제와 인공 지능의 발전사</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망(ANN, Aritificial Neural Networks)은 1943년 신경생리학자 Warren McCulloch과 수학자 Walter Pitts가 'A Logical Calculus of Ideas Immanent In Nervous Activity' 처음 소개했습니다. 그 이후 1960년대까지 당싱에 등장한 인공 신경망을 통해 사람들은 지능을 가지 기계가 상당히 엄청난 일을 해낼 것이라 생각했습니다. 그러나, 위의 그림(출처: [beamandrew's blog](https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html))처럼 사람들의 기대와는 달리 인공 신경망으로 XOR문제를 해결할 수 없게 되었고, 인공 지능과 관련한 연구는 암흑기로 접어 들게 되었다. 그래도, 1990년 대에는 SVM과 성능이 좋은 다른 머신러닝 알고리즘들이 나올 정도도 꾸준한 연구가 진행은 되고 있었습니다. \n",
    "\n",
    "2000년대에 들어서면서 인공 신경망은 2012년 ILSVRC2012 대회에서 인공 신경망을 깊게 쌓은 딥러닝 모델인 AlexNet이 압도적인 성적으로 우승하면서 다시금 주목받게 되었습니다. 이렇게 인공 신경망에 기반을 둔 딥러닝이 다시 주목받게 된 계기가 되었습니다. \n",
    "\n",
    "인공 지능의 발전사에서 약간의 의미가 있었던 XOR 문제를 아래 그림과 같은 다층 신경망을 통해 이제 어렵지 않게 풀어볼 수 있는 문제가 되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-xor.png?raw=true\" width=\"600\">\n",
    "<center>그림 3:  XOR 연산을 위한 다층 인공 신경망</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises \n",
    "\n",
    "#### AND, NAND, OR, NOR 게이트 로직을 다룰 수 있는 Gate라는 클래스를 만들고 출력을 확인하십시오. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/truthtable.png?raw=true\" width=\"400\">\n",
    "<center>그림 1:  OR, AND, NAND, XOR 진리표 </center>\n",
    "\n",
    "#### Part 1:  Gate 클래스 정의하기 (sse 인스턴스 변수와 코드 포함)\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gate:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([1.0, 1.0])         \n",
    "        self.b = np.array([1.0]) \n",
    "        self.eta = 0.1\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z = np.sum(x * self.w) + self.b\n",
    "        a = self.activation(z)\n",
    "        #print('forpass x{} z{} a{}'.format(x, z, a))\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, error): \n",
    "        w_grad = x * error\n",
    "        b_grad = 1 * error\n",
    "        return w_grad, b_grad\n",
    "    \n",
    "    def activation(self, z): \n",
    "        a = 1/(1 + np.exp(-z))\n",
    "        return a\n",
    "\n",
    "    def fit(self, x, y, epochs = 1000):         # default epochs = 1000\n",
    "        self.sse = np.zeros(epochs)             # initialize sse array as 0\n",
    "        \n",
    "        for i in range(epochs):                  # epoch만큼 반복합니다. \n",
    "            sse_i = 0                            # compute sse per epoch \n",
    "            \n",
    "            for x_i, y_i in zip(x, y):          # 모든 샘플에 대해 반복합니다.\n",
    "                a =                              # 순전파 계산\n",
    "                error =                          # 오차 계산\n",
    "                w_grad, b_grad =                 # 역방향 계산\n",
    "                self.w +=                        # 가중치 조정\n",
    "                self.b +=                        # 편향 조정\n",
    "                sse_i +=                         # accumulate error for each sample\n",
    "            self.sse[i] =                        # save each sse for every epoch in the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 입력 x와 y_and, y_nand, y_or, y_xor 레이블(타깃)을 정의하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y_and  = np.array([[0], [0], [0], [1]])\n",
    "y_nand = np.array([[1], [1], [1], [0]])\n",
    "y_or   = np.array([[0], [1], [1], [1]])\n",
    "y_nor  = np.array([[1], [0], [0], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: gate_and 객체와 훈련하기  (epochs = 500)\n",
    "\n",
    "- gate_and 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "AND Gate\n",
    "Weight: [4.31247748 4.30140545]\n",
    "  Bias: [-6.6310874]\n",
    "[0 0] [0] y_hat:[0.00131699]\n",
    "[0 1] [0] y_hat:[0.08869437]\n",
    "[1 0] [0] y_hat:[0.08959338]\n",
    "[1 1] [1] y_hat:[0.87897885]\n",
    "```\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate\n",
      "Weight: [6.98823409 6.9851074 ]\n",
      "  Bias: [-10.64790991]\n",
      "[0 0] [0] y_hat:[2.37498662e-05]\n",
      "[0 1] [0] y_hat:[0.02501851]\n",
      "[1 0] [0] y_hat:[0.02509489]\n",
      "[1 1] [1] y_hat:[0.96529103]\n"
     ]
    }
   ],
   "source": [
    "## your code here\n",
    "\n",
    "\n",
    "print(\"AND Gate\")\n",
    "print(\"Weight:\", gate_and.w)\n",
    "print(\"  Bias:\", gate_and.b)\n",
    "\n",
    "for i in range(len(x)): \n",
    "    print('your code here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: epoch에 따른 mse 시각화 하기\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-gate-mse.png?raw=true\" width=\"400\">\n",
    "<center>그림 4: Gradient Descent MSE for Logic Gate</center>\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('your code here')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Gradient Descent MSE for Logic Gate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: NAND Gate\n",
    "\n",
    "- gate_nand 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "NAND Gate\n",
    "Weight: [-4.25914936 -4.24781832]\n",
    "  Bias: [6.55111244]\n",
    "[0 0] [1] y_hat:[0.99857351]\n",
    "[0 1] [1] y_hat:[0.90914949]\n",
    "[1 0] [1] y_hat:[0.90820923]\n",
    "[1 1] [0] y_hat:[0.12391631]\n",
    "```\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND Gate\n",
      "Weight: [-6.97354402 -6.9703949 ]\n",
      "  Bias: [10.625858]\n",
      "[0 0] [1] y_hat:[0.99997572]\n",
      "[0 1] [1] y_hat:[0.97480184]\n",
      "[1 0] [1] y_hat:[0.97472437]\n",
      "[1 1] [0] y_hat:[0.03495609]\n"
     ]
    }
   ],
   "source": [
    "print('your code here')\n",
    "\n",
    "print(\"NAND Gate\")\n",
    "print(\"Weight:\", gate_nand.w)\n",
    "print(\"  Bias:\", gate_nand.b)\n",
    "\n",
    "for i in range(len(x)): \n",
    "    print('your code here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: OR Gate\n",
    "\n",
    "- gate_or 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 오차 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "OR Gate\n",
    "Weight: [5.42570445 5.43193129]\n",
    "  Bias: [-2.21461759]\n",
    "[0 0] [0] y_hat:[0.09844548]\n",
    "[0 1] [1] y_hat:[0.96148065]\n",
    "[1 0] [1] y_hat:[0.96124937]\n",
    "[1 1] [1] y_hat:[0.99982368]\n",
    "```\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Gate\n",
      "Weight: [8.22173119 8.22364178]\n",
      "  Bias: [-3.64329517]\n",
      "[0 0] [0] y_hat:[0.02549878]\n",
      "[0 1] [1] y_hat:[0.98985268]\n",
      "[1 0] [1] y_hat:[0.98983347]\n",
      "[1 1] [1] y_hat:[0.99999724]\n"
     ]
    }
   ],
   "source": [
    "print('your code here')\n",
    "\n",
    "print(\"OR Gate\")\n",
    "print(\"Weight:\", gate_or.w)\n",
    "print(\"  Bias:\", gate_or.b)\n",
    "for i in range(len(x)): \n",
    "    print('your code here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: NOR gate\n",
    "\n",
    "- gate_nor 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "NOR Gate\n",
    "Weight: [-5.33364922 -5.34053195]\n",
    "  Bias: [2.16668323]\n",
    "[0 0] [1] y_hat:[0.8972175]\n",
    "[0 1] [0] y_hat:[0.04016179]\n",
    "[1 0] [0] y_hat:[0.04042795]\n",
    "[1 1] [0] y_hat:[0.00020191]\n",
    "```\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOR Gate\n",
      "Weight: [-6.76459101 -6.76836513]\n",
      "  Bias: [2.9043104]\n",
      "[0 0] [1] y_hat:[0.9480591]\n",
      "[0 1] [0] y_hat:[0.02055152]\n",
      "[1 0] [0] y_hat:[0.02062763]\n",
      "[1 1] [0] y_hat:[2.42118118e-05]\n"
     ]
    }
   ],
   "source": [
    "print('your code here')\n",
    "\n",
    "print(\"NOR Gate\")\n",
    "print(\"Weight:\", gate_nor.w)\n",
    "print(\"  Bias:\", gate_nor.b)\n",
    "# print(\"   MSE:\", mse)\n",
    "for i in range(len(x)): \n",
    "    print('your code here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Logic Gate 만들기\n",
    "\n",
    "우리는 위에서 Gate 클래스를 사용하여, gate_and, gate_or, gate_nand, gate_nor과 다양한 논리회로 기능을 할 수 있는 객체를 만들어 낼 수 있었지만, 그러나 gate_xor 논리회로를 만들 수 없다는 것을 알게 되었습니다. 그런데, 이러한 논리회로들을 여러 개(layer)으로 조합하면, 아래 그림들과 같이 XOR 논리 회로를 만들 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/xor.png?raw=true\" width=\"600\">\n",
    "<center>그림 5:  XOR 연산자 만들기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 활용하여 gate_xor() 함수를 두 가지 방법으로 만들어 보십시오. gate_xor 객체 혹은 클래스를 만드는 것은 아니라는 것을 유의하십시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: gate_nand, gate_or, gate_and 객체 테스트 하기\n",
    "\n",
    "여기서는 gate_xor()에서 사용할 gate_nand, gate_or, gate_and 객체들이 제대로 작동하는지 확인하십시오. 다음은 각 객체들의 결과 값들의 한 예입니다. 구체적인 값들을 다를 수 있지만, 1 혹은 0에 가까운 값으로 산출되어야 합니다.\n",
    "\n",
    "```\n",
    "NAND [0 0] [0.99997572]\n",
    "NAND [0 1] [0.97480184]\n",
    "NAND [1 0] [0.97472437]\n",
    "NAND [1 1] [0.03495609]\n",
    "  OR [0 0] [0.02549878]\n",
    "  OR [0 1] [0.98985268]\n",
    "  OR [1 0] [0.98983347]\n",
    "  OR [1 1] [0.99999724]\n",
    " AND [0 0] [2.37498662e-05]\n",
    " AND [0 1] [0.02501851]\n",
    " AND [1 0] [0.02509489]\n",
    " AND [1 1] [0.96529103]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND [0 0] [0.99997572]\n",
      "NAND [0 1] [0.97480184]\n",
      "NAND [1 0] [0.97472437]\n",
      "NAND [1 1] [0.03495609]\n",
      "  OR [0 0] [0.02549878]\n",
      "  OR [0 1] [0.98985268]\n",
      "  OR [1 0] [0.98983347]\n",
      "  OR [1 1] [0.99999724]\n",
      " AND [0 0] [2.37498662e-05]\n",
      " AND [0 1] [0.02501851]\n",
      " AND [1 0] [0.02509489]\n",
      " AND [1 1] [0.96529103]\n"
     ]
    }
   ],
   "source": [
    "# gate_xor()에서 사용할 gate_nand, gate_or, gate_and 객체 기능 확인하기\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "\n",
    "for i in range(len(x)): \n",
    "    yhat = gate_nand.forpass(x[i])\n",
    "    print(\"NAND\", x[i], yhat)\n",
    "\n",
    "for i in range(len(x)): \n",
    "    yhat = gate_or.forpass(x[i])\n",
    "    print(\"  OR\", x[i], yhat)\n",
    "\n",
    "for i in range(len(x)): \n",
    "    yhat = gate_and.forpass(x[i])\n",
    "    print(\" AND\", x[i], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: gate_xor() 함수 구현하기 \n",
    "\n",
    "위의 그림에 있는 논리회로를 활용하여 gate_xor()함수를 구현하고 테스트해서 확인하십시오. \n",
    "```\n",
    " XOR [0 0] [0.02983645]\n",
    " XOR [0 1] [0.95599548]\n",
    " XOR [1 0] [0.95596705]\n",
    " XOR [1 1] [0.03172103]\n",
    "```\n",
    "\n",
    "참고로, 아래는 gate_xor()함수 안에서 디버깅을 위해 출력한 입력 x, 중간값 h1, h2, h12 및 출력 yhat입니다.  \n",
    "\n",
    "```\n",
    " XOR [0 0] [0.99997572] [0.02549878] [0.99997572 0.02549878] [0.02983645]\n",
    " XOR [0 1] [0.97480184] [0.98985268] [0.97480184 0.98985268] [0.95599548]\n",
    " XOR [1 0] [0.97472437] [0.98983347] [0.97472437 0.98983347] [0.95596705]\n",
    " XOR [1 1] [0.03495609] [0.99999724] [0.03495609 0.99999724] [0.03172103]\n",
    "```\n",
    "\n",
    "Note: forpass에 입력하는 값의 형상(shape) 혹은 차원(dim)에 유의하십시오. forpass()가 반환하는 값, 예를 들면, h1은 scalar가 아니라 ndarray type(형식)이고, forpass() 메소드에 넘겨주어야 하는 값의 type은 `x[1] = [0, 1]`와 같이 1차원의 1x2의 형상(shape)인 ndarray type입니다. 이를 위해 np.array()를 사용해 보는 것도 좋습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gate_xor() 함수 구현하기 \n",
    "# use gate_nand, gate_and and gate_or \n",
    "\n",
    "def gate_xor(x):\n",
    "    \n",
    "    print('your code here')\n",
    "    \n",
    "    #print(\" XOR\", x, h1, h2, h12, yhat)     # for debugging \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XOR [0 0] [0.02983645]\n",
      " XOR [0 1] [0.95599548]\n",
      " XOR [1 0] [0.95596705]\n",
      " XOR [1 1] [0.03172103]\n"
     ]
    }
   ],
   "source": [
    "# gate_xor() 함수 테스트하기\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "\n",
    "for i in range(len(x)): \n",
    "    yhat = gate_xor(x[i])\n",
    "    print(\" XOR\", x[i], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: gate_xor() 함수 구현하기 \n",
    "\n",
    "아래 그림에 있는 논리회로를 완성해 보십시오. 이를 활용하여 gate_xor()함수를 구현하고 테스트해서 확인하십시오. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/xor_nand_truthtable.png?raw=true\" width=\"400\">\n",
    "<center>그림 6: NAND로 구성된 XOR진리표 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림에서 각 Gate 앞에 있는 annotation을 변수 이름으로 사용하면 편리합니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/xor_nand_logic.png?raw=true\" width=\"400\">\n",
    "<center>그림 7: NAND로 구성된 XOR논리 회로 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 결과는 다음과 같습니다. 여러 분의 값들은 다를 수 있지만, 0 혹은 1에 다음과 같은 형태로 수렴해야 합니다. \n",
    "\n",
    "```\n",
    " XOR [0 0] [0.02983645]\n",
    " XOR [0 1] [0.95599548]\n",
    " XOR [1 0] [0.95596705]\n",
    " XOR [1 1] [0.03172103]\n",
    "```\n",
    "\n",
    "참고로, 아래는 gate_xor()함수 안에서 디버깅을 위해 출력한 입력 x, 중간값 h1, x1h1, h2, h3 및 출력 yhat입니다.  \n",
    "\n",
    "```\n",
    " XOR [0 0] [0.99997572] [0.         0.99997572] [0.97480599] [0.97472854] [0.04897468]\n",
    " XOR [0 1] [0.97480184] [0.         0.97480184] [0.97877463] [0.04139336] [0.97102171]\n",
    " XOR [1 0] [0.97472437] [1.         0.97472437] [0.04141164] [0.97872202] [0.97111137]\n",
    " XOR [1 1] [0.03495609] [1.         0.03495609] [0.96797397] [0.96806805] [0.05354669]\n",
    "```\n",
    "\n",
    "Note: forpass에 입력하는 값의 형상(shape) 혹은 차원(dim)에 유의하십시오. forpass()가 반환하는 값, 예를 들면, h1은 scalar가 아니라 ndarray type(형식)이고, forpass() 메소드에 넘겨주어야 하는 값의 type은 `x[1] = [0, 1]`와 같이 1차원의 1x2의 형상(shape)인 ndarray type입니다. 이를 위해 np.array()를 사용해 보는 것도 좋습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gate_xor() 함수 구현하기 \n",
    "# use gate_nand only\n",
    "\n",
    "def gate_xor(x):\n",
    "    \n",
    "    print('your code here')\n",
    "    \n",
    "    #print(\" XOR\", x, h1, x1h1, h2, h3, yhat)     # for debugging \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XOR [0 0] [0.02983645]\n",
      " XOR [0 1] [0.95599548]\n",
      " XOR [1 0] [0.95596705]\n",
      " XOR [1 1] [0.03172103]\n"
     ]
    }
   ],
   "source": [
    "# gate_xor() 함수 테스트하기\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "\n",
    "for i in range(len(x)): \n",
    "    yhat = gate_xor(x[i])\n",
    "    print(\" XOR\", x[i], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: yet another yetano_xor() 함수 구현하기 \n",
    "\n",
    "아래 그림에 있는 논리회로를 완성해 보십시오. 이를 활용하여 gate_xor()함수를 구현하고 테스트해서 확인하십시오. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/xor_nor_truthtable.png?raw=true\" width=\"400\">\n",
    "<center>그림 8: NOR로 구성된 XOR진리표 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림에서 각 Gate 앞에 있는 annotation을 변수 이름으로 사용하면 편리합니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/xor_nor_logic.png?raw=true\" width=\"400\">\n",
    "<center>그림 8: NOR로 구성된 XOR진리표 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 결과는 다음과 같습니다. 여러 분의 값들은 다를 수 있지만, 0 혹은 1에 다음과 같은 형태로 수렴해야 합니다. \n",
    "\n",
    "```\n",
    " XOR [0 0] [0.02894936]\n",
    " XOR [0 1] [0.92881756]\n",
    " XOR [1 0] [0.92882857]\n",
    " XOR [1 1] [0.0290579]\n",
    "```\n",
    "\n",
    "참고로, 아래는 yetano_xor()함수 안에서 디버깅을 위해 출력한 입력 x, 중간값 x11, x12, h12, h43 및 출력 yhat입니다.  \n",
    "\n",
    "```\n",
    " XOR [0 0] [0 0] [0 0] [0.9480591] [0.9480591 0.9480591] [4.88976716e-05 9.48059104e-01] [0.02894936]\n",
    " XOR [0 1] [0 0] [0 1] [2.42118118e-05] [9.48059104e-01 2.42118118e-05] [0.02905482 0.02055152] [0.92881756]\n",
    " XOR [1 0] [1 1] [1 0] [0.9480591] [2.42118118e-05 9.48059104e-01] [0.02895405 0.02062763] [0.92882857]\n",
    " XOR [1 1] [1 1] [1 1] [2.42118118e-05] [2.42118118e-05 2.42118118e-05] [9.48042967e-01 2.42118118e-05] [0.0290579]\n",
    "```\n",
    "\n",
    "Note: forpass에 입력하는 값의 형상(shape) 혹은 차원(dim)에 유의하십시오. forpass()가 반환하는 값, 예를 들면, h1은 scalar가 아니라 ndarray type(형식)이고, forpass() 메소드에 넘겨주어야 하는 값의 type은 `x[1] = [0, 1]`와 같이 1차원의 1x2의 형상(shape)인 ndarray type입니다. 이를 위해 np.array()를 사용해 보는 것도 좋습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yet another xor() 함수 구현하기 \n",
    "# use gate_nor only\n",
    "\n",
    "def yetano_xor(x):\n",
    "    print('your code here')\n",
    "    \n",
    "    #print(\" XOR\", x, x11, x12, h2, h12, h43, yhat)     # for debugging \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XOR [0 0] [0.02894936]\n",
      " XOR [0 1] [0.92881756]\n",
      " XOR [1 0] [0.92882857]\n",
      " XOR [1 1] [0.0290579]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "\n",
    "for i in range(len(x)): \n",
    "    yhat = yetano_xor(x[i])\n",
    "    print(\" XOR\", x[i], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
