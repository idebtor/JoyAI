{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 7-2 강: 순방향 신경망 실습$^{feed-forward \\ neural \\ network \\ practice}$\n",
    "- 순방향 신경망 신호처리 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'joy' from 'C:\\\\GitHub\\\\JoyAI\\\\deep_class\\\\joy.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import imp\n",
    "import joy\n",
    "imp.reload(joy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 다층 신경망의 신호처리\n",
    "\n",
    "이전 강의에서 여러분들은 다층 신경망에서 신호가 어떻게 처리되는지 직접 계산은 해보았습니다.\n",
    "하지만, 아직 실감이 가지 않을 수도 있습니다. 꿈의 함수 제조기가 겨우 행렬의 곱셈과 덧셈 몇 번만으로 끝나는 것인지 의심스러울 수도 있습니다. 왜냐하면 여러분들이 이전 강의에서 다룬 내용은 신경망에서 신호가 어떻게 처리 되는지를 확인 하기 위한 예제에서 그쳤기 때문입니다.\n",
    "\n",
    "우리는 이번 강의에서 인공 신경망이 정말 유의미한 값을 우리에게 알려주는지 정말 꿈의 함수 제조기가 맞는지 확인해 보도록 하겠습니다.\n",
    "\n",
    "아래는 오늘 우리가 실습해 볼 입력 자료 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MNIST50419.png\" width=\"600\">\n",
    "<center>그림 1: MNIST 데이터셋의 첫 5장의 이미지</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 라는 데이터셋의 첫 5장의 이미지입니다. MNIST 데이터 셋에 대한 자세한 설명은 뒤 강의에서 설명할테니 잠시 미루도록 하겠습니다.\n",
    "해당 이미지는 28x28의 크기를 가지고 있는 숫자 이미지 입니다. <br>저희가 보는 것과 같이 5, 0, 4, 1, 9가 적힌 이미지이며 우리는 오늘 이 숫자들을 한번 인식해보도록 하겠습니다.<br> \n",
    "바로 이전 강의에서 배웠던 행렬의 곱셈과 덧셈만으로 말이죠!\n",
    "\n",
    "벌써 두근거리지 않나요? 자, 하나씩 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 순방향 신경망을 계산해보기 위해 우리가 만들었던 3개의 층에 각각 2개, 3개, 2개의 뉴런이 구성된 신경망입니다.<br>\n",
    "$x_1$ 과 $x_2$는 인공 신경망의 입력 값이었고,<br>\n",
    "$W$는 두 층 사이의 가중치 행렬,<br>\n",
    "$Z$는 뉴론의 입력과 가중치를 곱한 결과의 합,<br>\n",
    "$A$는 뉴론의 출력으로 나타내는 것을 지난 시간에 약속하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2-3-2NN-Weights.png\" width=\"600\">\n",
    "<center>그림 2: 신경망의 입력과 출력 및 가중치 $W_{ij}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면, 숫자를 인식하기 위해 이번 실습에서도 입력은 $x1$과 $x2$만 있으면 될까요? 그렇지 않습니다. 왜냐하면, 우리가 인식할 숫자 이미지는 28x28의 크기를 가지고 있는 이미지입니다.<br>\n",
    "28x28 크기의 숫자 이미지를 인식하기 위해서는 784($28*28$)개의 입력이 필요합니다.\n",
    "즉, $X$는 $x_1, x_2, x_3, ..., x_{784}$가 되어야 합니다. 왜냐하면, 28x28의 이미지 각각의 화소(piexel)들이 하나의 뉴론으로 신경망에 주어지기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/28x28mnist.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조금 감이 잡히나요?\n",
    "\n",
    "그렇다면 은닉층을 구성하는 뉴론은 몇개가 좋을까요? 정답은 없습니다. 개발자들이 정해주기 나름입니다. 물론 입력 뉴론이 많아지면 은닉층의 뉴론도 같이 늘려줘야 학습이 제대로 이루어 집니다. 이번 실습에서는 은닉층의 뉴론을 100개로 설정하도록 하겠습니다.\n",
    "\n",
    "벌써 2개의 층을 구성하였습니다. 마지막으로 출력층의 뉴론은 몇개로 구성을 해야 할까요?<br>\n",
    "우리는 숫자를 인식하는 인공신경망을 만들고자 합니다. 따라서 0 ~ 9 사이의 숫자 총 10가지를 인식할 것입니다. 우리가 만든 신경망이 0 ~ 9 까지의 숫자를 인식할 수 있다면, 3 자리 수, 4 자리 수, 그 이상의 수도 쉽게 인식 할 수 있겠죠?\n",
    "\n",
    "이제, 우리가 만든 신경망을 도식화하여 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/784-100-10NN.png\" width=\"600\">\n",
    "<center>그림 3: 숫자 인식 인공 신경망</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 2의 신경망에 비해 크게 달라진 점은 각 층의 뉴론의 개수가 많아진 것과 출력이 2개에서 10개로 늘어난 것 외에는 없나요? 한 가지 빠진게 있죠? 바로 인공 신경망의 핵심이라고 할 수 있는 가중치도 입력 뉴론과 은닉 뉴론이 많아짐에 따라 같이 증가하였습니다. \n",
    "\n",
    "과연 인공 신경망의 가중치는 몇개나 많아졌을까요?\n",
    "\n",
    "입력 뉴론이 784개이고, 첫 번째 은닉층의 뉴론이 100개입니다. 각각의 입력 뉴론이 은닉층의 뉴론들과 연결되어 있으므로 총 784 x 100 개의 서로 다른 가중치를 가지게 됩니다. 놀라울 정도로 많지 않습니까?<br>\n",
    "출력 층은 0~9까지의 숫자를 인식하기 때문에 10개의 출력 뉴론이 될 것이기에 첫 번째 은닉층의 뉴론 100개와 출력 층 뉴론의 개수 10개의 조합인 100 x 10 개의 가중치를 가지게 될 것입니다.\n",
    "뉴론의 개수가 많아졌다고 해서 겁 먹을 필요는 전혀 없습니다. 우리는 이 많은 가중치 합을 직접 계산 하는 것이 아니라 컴퓨터가 대신 해줄 것을 알기 때문이죠. \n",
    "\n",
    "지금까지 우리는 숫자를 인식하는 인공 신경망을 만들기 위해 각 층의 뉴론들이 어떻게 구성되어야 하는지 살펴보았습니다. 우리가 생각한 인공 신경망이 제대로 동작하는지 코드로 확인해보도록 할까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 숫자 인식 인공 신경망 구현\n",
    "### 2.1 입력 자료 준비\n",
    "\n",
    "입력 자료는 MNIST 데이터 셋에 있는 숫자 이미지를 사용할 것입니다.\n",
    "\n",
    "신경망의 입력 특성 행렬$^{feature \\ matrix}, \\mathbf{X}^{n\\times m}$은 1개의 샘플$(m=1)$과 784개의 특성$(n=784)$를 가지고 있으므로 다음과 같이 표기할 수 있습니다.\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{X} \\in  \\mathbb{R}^{nxm} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "   x^{(0)}_1\\\\\n",
    "   x^{(0)}_2\\\\\n",
    "   \\vdots\\\\\n",
    "   x^{(0)}_{783}\\\\\n",
    "   x^{(0)}_{784}\n",
    "\\end{pmatrix} \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 가중치 불러오기\n",
    "지난 강의에서는 가중치를 임의의 값으로 초기화 하였습니다. 하지만, 오늘 강의에서는 미리 학습된 인공 신경망의 가중치 값을 불러와서 사용할 것입니다. 이 가중치 값은 우리가 만들어가고 있는 다층 인공 신경망의 구조와 동일한 구조로 만들어진 인공 신경망을 학습시켜서 얻은 값입니다. 약 96%의 성능을 보이는 인공 신경망의 가중치 값입니다.\n",
    "\n",
    "우리가 코드로 직접 확인해볼 가중치는 784 x 100로 첫번째 은닉층의 가중치를 열어서 확인해 보겠습니다.\n",
    "우리가 배운 표기법으로 나타내면 첫번째 은닉층의 가중치는 다음과 같습니다.\n",
    "\n",
    "\\begin{align}\n",
    "  W^{[2]}\\ &= \n",
    "  \\begin{pmatrix}\n",
    "   w^{(1)}_1 & w^{(2)}_1  & \\cdots & W^{(99)}_1 & w^{(100)}_1\\\\\n",
    "   w^{(1)}_2 & w^{(2)}_2  & \\cdots & W^{(99)}_2 & w^{(100)}_2\\\\\n",
    "   \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "   w^{(1)}_{100} & w^{(2)}_{100}  & \\cdots & w^{(99)}_{100} & w^{(100)}_{100}\n",
    "\\end{pmatrix} \n",
    "\\end{align}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!type data/w_xh.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!type data/w_xh.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!cat data/w_xh.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 은닉층 계산\n",
    "\n",
    "이제 첫번째 은닉층의 가중치도 확인해 봤으니 입력 값과 가중치를 코드로 변환하고, 행렬의 곱과 합으로 첫번째 층의 출력을 계산해보겠습니다.\n",
    "\n",
    "\n",
    "먼저, 지난 강의에서 배웠던 입력 뉴론이 2개, 은닉 뉴론이 3개 출력 뉴론이 2개였던 인공 신경망의 가중치 합$^{weighted \\ sum}$은 일반 식(1)을 적용하여 다음과 같이 신경망의 결과를 만들었습니다.\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{Z}^{[l]} &= W^{[l]} A^{[l-1]}    \\tag{1} \\\\ \\\\\n",
    "  \\mathbf{Z}^{[1]} &= W^{[1]} A^{[0]}   \\\\\n",
    "                   &= \n",
    "  \\begin{pmatrix}\n",
    "    w^{(1)}_{11} & w^{(1)}_{21} \\\\  \n",
    "    w^{(1)}_{12} & w^{(1)}_{22} \\\\\n",
    "    w^{(1)}_{13} & w^{(1)}_{23}\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix} x_1  \\\\ x_2  \\\\ \\end{pmatrix} \\\\ &=\n",
    "  \\begin{pmatrix} z^{(1)}_1  \\\\ z^{(1)}_2  \\\\ z^{(1)}_3 \\end{pmatrix} \\\\\n",
    "\\end{align}\n",
    "\n",
    "숫자를 인식하는 인공 신경망에서는 각 층 뉴론의 개수만 늘어났기 때문에 식(1)에서 나타나는 열의 숫자는 1 ~ 784으로 행의 숫자는 1 ~ 100 으로 확장 된 식을 생각하시면 됩니다.\n",
    "\n",
    "\n",
    "계산 식은 위에서 말한 식(1)을 따르도록 하겠습니다.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{W}^{[1]} \\mathbf{X}^{[0]} = \n",
    "\\begin{pmatrix}\n",
    "w^{(1)}_1 & w^{(2)}_1  & \\cdots & w^{(783)}_1 & w^{(784)}_1\\\\\n",
    "w^{(1)}_2 & w^{(2)}_2  & \\cdots & w^{(783}_2 & w^{(784)}_2\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "w^{(1)}_{100} & w^{(2)}_{100}  & \\cdots & w^{(783)}_{100} & w^{(784)}_{100}\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "   x^{(0)}_1\\\\\n",
    "   x^{(0)}_2\\\\\n",
    "   \\vdots\\\\\n",
    "   x^{(0)}_{783}\\\\\n",
    "   x^{(0)}_{784}\n",
    "\\end{pmatrix}\\\\ \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}^{[l]} &=\\mathbf{g}(\\mathbf{Z}^{[l]})\\\\\n",
    "\\mathbf{A}^{[1]} &=sigmoid(\\mathbf{Z}^{[1]})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, MNIST 숫자 데이터를 불러오기 위해 joy 모듈을 import 해줍니다. joy 모듈에 대해서는 뒤 강의에서 자세히 설명하도록 하겠습니다. 우선은 신경망이 정말로 제대로 동작하는지 부터 한번 확인해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joy\n",
    "import numpy as np\n",
    "g = lambda x : 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 7\n",
      "X.shape: (784,)\n",
      "W1.shape: (100, 784)\n",
      "Z1.shape: (100,)\n",
      "A1.shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "(X, y) = joy.load_mnist_num(7)\n",
    "W1 = joy.load_mnist_weight('data/w_xh.weights')\n",
    "Z1 = np.dot(W1, X)\n",
    "A1 = g(Z1)\n",
    "\n",
    "print('image', y)\n",
    "print('X.shape:', X.shape)\n",
    "print('W1.shape:', W1.shape)\n",
    "print('Z1.shape:', Z1.shape)\n",
    "print('A1.shape:', A1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$는 joy.load_num(5)로 불러들인 5라는 숫자가 적힌 이미지이고 $y$는 불러들인 이미지의 레이블 입니다.<br>\n",
    "$W1$은 미리 학습된 가중치 이며 100 X 784의 형상을 가지고 있습니다.<br>\n",
    "$Z1$은 첫번째 은닉층의 결과이며 이를 활성화 함수$^{sigmoid}$ $g(\\cdot)$를 통해 출력 값 $A1$을 만들어 냅니다.\n",
    "\n",
    "여러분들은 이번 실습이 끝나고 joy.load_num() 함수를 통해 다른 숫자들도 불러와 인공 신경망을 확인해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 출력층 계산\n",
    "이제 마지막으로 출력층을 계산하는 코드를 작성해 보겠습니다. 앞에서 배운 것과 같이 은닉층의 출력 $A^{[1]}$과 가중치 $W^{[2]}$ 행렬을 곱하고 합산하여 은닉층의 결과를 구한 다음, 활성화 함수를 적용해 최종 출력 $A^{[2]}$를 구할 수 있습니다. 이렇게 구한 결과 값이 신경망이 예측한 값 $\\hat{y}$ 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\mathbf{Z}^{[l]} &= W^{[l]} A^{[l-1]}   \\tag{2}\\\\ \\\\\n",
    "  \\mathbf{Z}^{[2]} &= W^{[2]} A^{[1]}   \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  W^{[2]}A^{[1]}\\ &= \n",
    "  \\begin{pmatrix}\n",
    "   w^{(1)}_1 & w^{(2)}_1  & \\cdots & w^{(99)}_1 & w^{(100)}_1\\\\\n",
    "   w^{(1)}_2 & w^{(2)}_2  & \\cdots & w^{(99)}_2 & w^{(100)}_2\\\\\n",
    "   \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "   w^{(1)}_{10} & w^{(2)}_{10}  & \\cdots & w^{(99)}_{10} & w^{(100)}_{10}\n",
    "   \\end{pmatrix} \n",
    "   \\begin{pmatrix}\n",
    "   a^{(1)}_1\\\\\n",
    "   a^{(1)}_2\\\\\n",
    "   \\vdots \\\\\n",
    "   a^{(1)}_{99}\\\\\n",
    "   a^{(1)}_{100}\n",
    "   \\end{pmatrix} \n",
    "\\end{align}  \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}^{[l]} &=\\mathbf{g}(\\mathbf{Z}^{[l]})\\\\\n",
    "\\mathbf{A}^{[2]} &=sigmoid(\\mathbf{Z}^{[2]})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1.shape: (100,)\n",
      "W2.shape: (10, 100)\n",
      "Z2.shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "W2 = joy.load_mnist_weight('data/w_hy.weights')\n",
    "Z2 = np.dot(W2, A1)\n",
    "yhat = g(Z2)\n",
    "print('A1.shape:', A1.shape)\n",
    "print('W2.shape:', W2.shape)\n",
    "print('Z2.shape:', Z2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에 적힌 숫자와 인공 신경망이 예측한 값이 서로 맞는지 확인해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: 7\n",
      "predict: [0.    0.002 0.001 0.    0.    0.001 0.    0.979 0.006 0.003]\n"
     ]
    }
   ],
   "source": [
    "print('image:', y)\n",
    "print('predict:', np.round_(yhat, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예상했던 결과 값인가요? 이번 결과 값은 약간의 해석이 필요할 것 같습니다.\n",
    "\n",
    "인공 신경망이 예측한 값은 정확히 우리가 눈으로 보는 것처럼 \"5\" 이런 식으로 정답을 알려주는 것이 아닙니다.<br>\n",
    "우리가 인공 신경망의 구조를 만들때를 한번 다시 생각해 볼까요?<br>\n",
    "출력 뉴론의 개수를 총 10개로 지정하였던 것을 기억하시나요? 그 이유는 우리가 인식하고자 하는 숫자는 0 ~ 9 까지 총 10가지이기 때문입니다.\n",
    "\n",
    "이 처럼 인공 신경망이 생각하기에 입력과 가장 비슷한 숫자를 찾아 확률로 결과를 나타냅니다.\n",
    "$\\hat{y}$은 총 10개의 행렬로 정답을 알려주는데, 컴퓨터 프로그래밍에서 행렬은 0부터 시작하기 때문에 각각의 색인$^{index}$ 은 0 ~ 9까지의 숫자일 확률을 의미 합니다. 예를 들어, 행렬의 0번째 원소 값은 숫자 0일 확률을 나타내는 것이지요.\n",
    "\n",
    "그렇다면 0부터 하나씩 행렬의 색인$^{index}$을 세어보면 4번째 원소의 값이 0.997로 가장 높은 값을 보여주고 있음을 알 수 있습니다. 다시 말해, 인공 신경망도 우리가 보여준 숫자가 5라고 말하고 있는 것이지요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 5 말고 다른 숫자 1도 인식하는지 한번 확인해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: 1\n",
      "predict: [0.001 0.976 0.009 0.001 0.021 0.017 0.001 0.004 0.005 0.001]\n",
      "[0] = 0.001\n",
      "[1] = 0.976\n",
      "[2] = 0.009\n",
      "[3] = 0.001\n",
      "[4] = 0.021\n",
      "[5] = 0.017\n",
      "[6] = 0.001\n",
      "[7] = 0.004\n",
      "[8] = 0.005\n",
      "[9] = 0.001\n",
      "image=1, predicted=0.976\n"
     ]
    }
   ],
   "source": [
    "(X, y) = joy.load_mnist_num(1)\n",
    "W1 = joy.load_mnist_weight('data/w_xh.weights')\n",
    "Z1 = np.dot(W1, X)\n",
    "A1 = g(Z1)\n",
    "\n",
    "W2 = joy.load_mnist_weight('data/w_hy.weights')\n",
    "Z2 = np.dot(W2, A1)\n",
    "yhat = g(Z2)\n",
    "\n",
    "print('image:', y)\n",
    "print('predict:', np.round_(yhat, 3))\n",
    "\n",
    "for i, iyhat in enumerate(yhat):\n",
    "    print('[{}] = {}'.format(i, np.round(yhat[i], 3)))\n",
    "\n",
    "idx = np.argmax(yhat)\n",
    "print('image={}, predicted={}'.format(idx, np.round(yhat[idx], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "놀라울 정도로 1번째 색인$^{index}$값이 0.979로 높음을 확인 할 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 지금까지 한 것은 여러분들이 나중에 기계학습을 활용할 때 직접 가중치를 학습하여서 사용할 수도 있지만, 때로는 다른 사람이 학습한 가중치를 불러와 사용할 때도 있습니다.<br>\n",
    "여러분들이 이번 강의에서 했던 것 처럼 말이죠!\n",
    "\n",
    "우리는 이제 순방향 신경망이 정말 잘 동작하고 있는 것을 숫자 인식을 통해 확인하였습니다.<br>\n",
    "이제 다음 강의부터는 제공된 가중치를 사용하는 것이 아니라 직접 가중치를 학습하는 알고리즘을 배워보도록 하겠습니다.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__JoyQuiz 1__\n",
    "\n",
    "MNIST 데이터셋의 첫 다섯 숫자는 5, 0, 4, 1, 9입니다. 이 숫자를 주어진 data/w_xh.weights 가중치를 사용하는 순방향 신경망으로 분류를 했을 때, 다른 숫자들과 비교하여 가장 정확도 낮은 숫자는 어느 것인가요?\n",
    "\n",
    "__JoyAnswer: 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x : 1 / (1 + np.exp(-x))   # 활성화 함수\n",
    "\n",
    "num_list = [5, 0, 4, 1, 9]            # 순방향 신경망에서 처리할 숫자들\n",
    "for i, num in enumerate(num_list):\n",
    "    (X, y) = joy.load_mnist_num(num)\n",
    "    W1 = joy.load_mnist_weight('data/w_xh.weights')\n",
    "    Z1 = np.dot(W1, X)\n",
    "    A1 = g(Z1)\n",
    "\n",
    "    W2 = joy.load_mnist_weight('data/w_hy.weights')\n",
    "    Z2 = np.dot(W2, A1)\n",
    "    yhat = g(Z2)\n",
    "    \n",
    "    max_idx = np.argmax(yhat)\n",
    "    print('image={}, probability={}'.format(max_idx, yhat[max_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__JoyQuiz 2__\n",
    "\n",
    "MNIST 데이터셋의 첫 다섯 숫자는 5, 0, 4, 1, 9입니다. 이 숫자들을 하나씩 data/w_xh.weights 가중치를 사용하는 순방향 신경망으로 분류를 하는 코드를 아래와 같이 완성하기 위하여 필요한 함수 이름을 빈칸에 채우십시오. \n",
    "\n",
    "힌트: 신경망의 예측값 yhat의 크기는 10인 배열입니다. 가장 큰 원소의 값을 가지고 있는 배열의 인덱스를 찾으면, 그것이 바로 신경망이 예측하는 숫자이며, 그 원소값이 최대 확률입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x : 1 / (1 + np.exp(-x))   # 활성화 함수\n",
    "\n",
    "num_list = [5, 0, 4, 1, 9]  # 순방향 신경망에서 처리할 숫자들\n",
    "for i, num in enumerate(num_list):\n",
    "    (X, y) = joy.load_mnist_num(num)\n",
    "    W1 = joy.load_mnist_weight('data/w_xh.weights')\n",
    "    Z1 = np.dot(W1, X)\n",
    "    A1 = g(Z1)\n",
    "    W2 = joy.load_mnist_weight('data/w_hy.weights')\n",
    "    Z2 = np.dot(W2, A1)\n",
    "    yhat = g(Z2)\n",
    "\n",
    "    max_index = np.argmax(yhat)\n",
    "    print('({}) class label:{}, predict:{}, probability:{}'.\n",
    "          format(i, num, max_index, yhat[max_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__JoyAnswer: np.argmax(yhat)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
